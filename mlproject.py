# -*- coding: utf-8 -*-
"""mlproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TL6yADWIG2gUWydOnHp1AgS3oy6_Gi58

#data set link :https://drive.google.com/drive/folders/1X7FGBuxx2405tv64OnxebB3fLq--5Ada?usp=sharing

#Matrix factorization
"""

#library imports
import numpy as np
import pandas as pd
from collections import Counter
from sklearn.model_selection import train_test_split
from scipy import sparse
#import jovian

movie_ratings_df = pd.read_csv("/content/ratings.csv")
movie_ratings_df.shape
print(movie_ratings_df.head())

#checking for null values
movie_ratings_df.isnull().sum()

#calculating number of each rating
Counter(movie_ratings_df.rating)

#distribution of userid with movies
movie_ratings_df.groupby(['userId']).count()['movieId']

#Average number of ratings per user
np.mean(movie_ratings_df.groupby(['userId']).count()['movieId'])

train_df, valid_df = train_test_split(movie_ratings_df, test_size=0.2)

#resetting indices to avoid indexing errors in the future
train_df = train_df.reset_index()[['userId', 'movieId', 'rating']]
valid_df = valid_df.reset_index()[['userId', 'movieId', 'rating']]

userId = np.array(movie_ratings_df['userId'])
movieId = np.array(movie_ratings_df['movieId'])

"""#Preprocessing

we need continuous IDs to be able to index into the embedding matrix and access each user/item embedding.
"""

def encode_column(column):
    """ Encodes a pandas column with continous IDs"""
    keys = column.unique()
    key_to_id = {key:idx for idx,key in enumerate(keys)}
    return key_to_id, np.array([key_to_id[x] for x in column]), len(keys)

def encode_df(movie_df):
    """Encodes rating data with continuous user and movie ids"""
    
    movieIds, movie_df['movieId'], num_movie = encode_column(movie_df['movieId'])
    userIds, movie_df['userId'], num_users = encode_column(movie_df['userId'])
    return movie_df, num_users, num_movie, userIds, movieIds

movie_df, num_users, num_movie, userIds, movieIds = encode_df(train_df)
print("Number of users :", num_users)
print("Number of movie :", num_movie)
movie_df.head()

"""#Training
Our goal is to find the optimal embeddings for each user and each item.

Initializing user and item embeddings
"""

def create_embeddings(n, K):
    """
    Creates a random numpy matrix of shape n, K with uniform values in (0, 11/K)
    n: number of items/users
    K: number of factors in the embedding 
    """
    return 11*np.random.random((n, K)) / K

"""Creating Sparse utility matrix"""

def create_sparse_matrix(df, rows, cols, column_name="rating"):
  return sparse.csc_matrix((df[column_name].values,(df['userId'].values, df['movieId'].values)),shape=(rows, cols))

movie_df, num_users, num_movie, userIds, movieIds = encode_df(train_df)
Y = create_sparse_matrix(movie_df, num_users, num_movie)
# to view matrix
Y.todense()

"""Making predictions"""

def predict(df, emb_user, emb_movie):
    """ This function computes df["prediction"] without doing (U*V^T).
    
    Computes df["prediction"] by using elementwise multiplication of the corresponding embeddings and then 
    sum to get the prediction u_i*v_j. This avoids creating the dense matrix U*V^T.
    """
    df['prediction'] = np.sum(np.multiply(emb_movie[df['movieId']],emb_user[df['userId']]), axis=1)
    return df

"""Cost function: We are trying to minimize the Mean squared error over the utility matrix. """

lmbda = 0.0001

def cost(df, emb_user, emb_movie):
    """ Computes mean square error"""
    Y = create_sparse_matrix(df, emb_user.shape[0], emb_movie.shape[0])
    predicted = create_sparse_matrix(predict(df, emb_user, emb_movie), emb_user.shape[0], emb_movie.shape[0], 'prediction')
    return np.sum((Y-predicted).power(2))/df.shape[0]

"""Gradient Descent"""

def gradient(df, emb_user, emb_movie):
    """ Computes the gradient for user and movie embeddings"""
    Y = create_sparse_matrix(df, emb_user.shape[0], emb_movie.shape[0])
    predicted = create_sparse_matrix(predict(df, emb_user, emb_movie), emb_user.shape[0], emb_movie.shape[0], 'prediction')
    delta =(Y-predicted)
    grad_user = (-2/df.shape[0])*(delta*emb_movie) + 2*lmbda*emb_user
    grad_movie = (-2/df.shape[0])*(delta.T*emb_user) + 2*lmbda*emb_movie
    return grad_user, grad_movie

def gradient_descent(df, emb_user, emb_movie, iterations=2000, learning_rate=0.001, df_val=None):
    """ 
    Computes gradient descent with momentum (0.9) for given number of iterations.
    emb_user: the trained user embedding
    emb_movie: the trained movie embedding
    """
    Y = create_sparse_matrix(df, emb_user.shape[0], emb_movie.shape[0])
    beta = 0.9
    grad_user, grad_movie = gradient(df, emb_user, emb_movie)
    v_user = grad_user
    v_movie = grad_movie
    for i in range(iterations):
        grad_user, grad_movie = gradient(df, emb_user, emb_movie)
        v_user = beta*v_user + (1-beta)*grad_user
        v_movie = beta*v_movie + (1-beta)*grad_movie
        emb_user = emb_user - learning_rate*v_user
        emb_movie = emb_movie - learning_rate*v_movie
        if(not (i+1)%50):
            print("\niteration", i+1, ":")
            print("train mse:",  cost(df, emb_user, emb_movie))
            if df_val is not None:
                print("validation mse:",  cost(df_val, emb_user, emb_movie))
    return emb_user, emb_movie

emb_user = create_embeddings(num_users, 3)
emb_movie = create_embeddings(num_movie, 3)
emb_user, emb_movie = gradient_descent(movie_df, emb_user, emb_movie, iterations=800, learning_rate=1)

"""Making predictions on new data"""

def encode_new_data(valid_df, userIds, movieIds):
    """ Encodes valid_df with the same encoding as train_df.
    """
    df_val_chosen = valid_df['movieId'].isin(movieIds.keys()) & valid_df['userId'].isin(userIds.keys())
    valid_df = valid_df[df_val_chosen]
    valid_df['movieId'] =  np.array([movieIds[x] for x in valid_df['movieId']])
    valid_df['userId'] = np.array([userIds[x] for x in valid_df['userId']])
    return valid_df

print("before encoding:", valid_df.shape)
valid_df = encode_new_data(valid_df, userIds, movieIds)
print("after encoding:", valid_df.shape)

train_mse = cost(train_df, emb_user, emb_movie)
val_mse = cost(valid_df, emb_user, emb_movie)
print(train_mse, val_mse)

#looking at the predictions
valid_df[1:10].head()

predicted_ratings = np.array(valid_df[['prediction']])
  true_ratings = np.array(valid_df[['rating']])

# importing the required module
import matplotlib.pyplot as plt

plt.scatter(movieId[:500],true_ratings[:500],label= "stars", color= "blue", marker= "*")
plt.scatter(movieId[:500],predicted_ratings[:500],label= "stars", color= "orange", marker= "*")
plt.title('Predicted vs Actual w.r.t MovieID')
plt.ylabel('ratings')
plt.xlabel('movieId')
plt.legend(['actual', 'predicted'], loc='upper left')
plt.show()

plt.scatter(userId[:500],true_ratings[:500],label= "stars", color= "blue", marker= "*")
plt.scatter(userId[:500],predicted_ratings[:500],label= "stars", color= "orange", marker= "*")
plt.title('Predicted vs Actual w.r.t UserID')
plt.ylabel('ratings')
plt.xlabel('userId')
plt.legend(['actual', 'predicted'], loc='upper left')
plt.show()

#evaluates on test set
from sklearn.metrics import mean_squared_error
def score_on_test_set():
#user_movie_pairs = zip(valid_df[‘movieId’], valid_df[‘userId’])
  predicted_ratings = np.array(valid_df[['prediction']])
  true_ratings = np.array(valid_df[['rating']])
  score = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))
  return score
test_set_score = score_on_test_set()
print(test_set_score)

""" Common latent factor techniques compute a low-rank rating matrix by applying Singular Value Decomposition through gradient descent
or Regularized Alternating Least Square algorithm
"""

!wget http://files.grouplens.org/datasets/movielens/ml-1m.zip

!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip

!unzip ml-1m.zip

!unzip ml-100k.zip

"""#User-Based Collaborative Filtering"""

import numpy as np
from scipy.sparse import csr_matrix
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import seaborn as sns

dataset = pd.read_csv('ml-100k/u.data', delimiter = '\t',names=['user_id','item_id','rating','timestamp'])
dataset.head()

"""Transforming data into the matrix"""

n_users = dataset.user_id.unique().shape[0]
n_items = dataset.item_id.unique().shape[0]
n_items = dataset['item_id'].max()
A = np.zeros((n_users,n_items))
for line in dataset.itertuples():
    A[line[1]-1,line[2]-1] = line[3]
print("Original rating matrix : ",A)

"""converts the MovieLens dataset into the binary MovieLens dataset. We have considered items whose ratings are greater or equal to 3 being liked by the user and others being disliked by the user."""

for i in range(len(A)):
  for j in range(len(A[0])):
    if A[i][j]>=3:
      A[i][j]=1
    else:
      A[i][j]=0

"""we convert the dense rating matrix into a sparse matrix using the csr_matrix() function."""

csr_sample = csr_matrix(A)
print(csr_sample)

"""Compute similarity between items of csr_sample using cosine similarity"""

knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=3, n_jobs=-1)
knn.fit(csr_sample)

"""generate recommendations for user_id:1 based on 20 items being liked by him"""

dataset_sort_des = dataset.sort_values(['user_id', 'timestamp'], ascending=[True, False])
filter1 = dataset_sort_des[dataset_sort_des['user_id'] == 1].item_id
filter1 = filter1.tolist()
filter1 = filter1[:20]
print("Items liked by user: ",filter1)

"""Next, for each item being liked by the user1, we recommend 2 similar items"""

distances1=[]
indices1=[]
for i in filter1:
  distances , indices = knn.kneighbors(csr_sample[i],n_neighbors=3)
  indices = indices.flatten()
  indices= indices[1:]
  indices1.extend(indices)
print("Items to be recommended: ",indices1)

"""This is to create a repos for movie rating prediction. It covers 3 use cases.

Predict user rating from 0 to 5 by creating a stacked auto-encoder netowrk.

Predict user likes or not by creating a restricted Boltzmann machine.

use Pearson correlation matrix and create an item-based movie recommender.

#Stacked_Autoencoder_Rating_Prediction.
"""

# AutoEncoders

# Importing the libraries
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
from torch.autograd import Variable

# Importing the dataset
movies = pd.read_csv('ml-1m/movies.dat', sep = '::', names=['title','genres'], engine = 'python', encoding = 'latin-1')
users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')
ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')

movies.head()

users.head()

ratings.head()

##create training and test set data
# here only take u1.base but there are another u2 u3 u4 u5 base files
training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\t', header = None)
print(training_set.head(3))

"""we convert Dataframe to Numpy array because we will use Pytorch tensor which requires array as input. """

##convert it to array
training_set = np.array(training_set, dtype = 'int')

training_set.shape

test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\t', header = None)
##convert it to array
test_set = np.array(test_set, dtype = 'int')
test_set.shape

"""Data structure creation

To prepare the training/test data, we need to create training/test sets in array format with each row representing a user and each cell in the row representing the rating for each movie. This is the expected input for auto-encoder.
"""

#take max users id in train and test data
nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))
nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))
print('Num of users: ', nb_users, '\nNum of movies: ', nb_movies)

"""We create a function for data conversion which returns a list of lists. Each child list represents one user’s ratings for all movies. If the user did not rate a movie, initialize the rating with 0. """

def convert(data):
    new_data = []
    for id_users in range(1, nb_users + 1):
        ##id of movies that is rated by current users
        id_movies = data[:,1][data[:,0] == id_users]
        
        ##rate of movies that is given by current user
        id_ratings = data[:,2][data[:,0] == id_users]
        
        #inialize ratings for all movies
        #set 0 for movies that are not rated by current users
        ratings = np.zeros(nb_movies)
        #movie id starts from 1, 1st movie will be 1st element in rating with index as 0
        ratings[id_movies - 1] = id_ratings
        new_data.append(list(ratings))
    return new_data

"""With the above conversion function, we convert the training set and test set."""

training_set = convert(training_set)
test_set = convert(test_set)

"""convert data into Torch tensor because we will use Pytorch to build the auto-encoder."""

training_set = torch.FloatTensor(training_set)
test_set = torch.FloatTensor(test_set)

training_set

test_set

""" SAE architecture creation"""

class SAE(nn.Module):
    def __init__(self, ):
        #allow to inhert all classes and methods of parent class
        super(SAE, self).__init__()
        #num of features from input: num of movies, 20 nodes in first hidden layer
        self.fc1 = nn.Linear(nb_movies, 20)
        self.fc2 = nn.Linear(20, 10)
        #start to decoding
        self.fc3 = nn.Linear(10, 20)
        self.fc4 = nn.Linear(20, nb_movies)
        self.activation = nn.Sigmoid()
        
    #input vector (movies rating) for a specific users
    def forward(self, x):
        #apply activaton fuc on first encoding layer
        #return first encoded vector
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.activation(self.fc3(x))
        #no need to activate the vector
        #output the reconstrctured vector
        x = self.fc4(x)
        return x

""" Model training"""

sae = SAE()
#Create loss fucn object
criterion = nn.MSELoss()
#create optimizer object
#parameters of all auto-encoders defined in the class
optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)

# #train the SAE using pytorch only codes

#loop all epochs
nb_epoch = 200
for epoch in range(1, nb_epoch + 1):
    train_loss = 0
    #exclude users who did not rate any movies
    #define a float
    s = 0.
    #loop through each users
    for id_user in range(nb_users):
        #get all rating for current user from training_set
        #nn does not take single dimension vector, so add a batch dimension
        #a batch of sinlge inptu vector, update weigths after each vector
        input = Variable(training_set[id_user]).unsqueeze(0)
        #create target by copying input
        target = input.clone()
        #only look at users who rated at least 1 movie
        if torch.sum(target.data > 0) > 0:
            #get output from the network, a vector of predicted value
            output = sae(input)
            #do not compute gradient with respect to target
            target.require_grad = False
            #don't account the output whose initial input is 0
            output[target == 0] = 0
            loss = criterion(output, target)
            #make demonitor is not zero, to add a small number
            mean_corrector = nb_movies / float(torch.sum(target.data>0) + 1e-10)
            #backward method to determine which direction 
            loss.backward()
            #access the data of loss object .data[0]
            #adjust the loss to compute relevant mean for all movies for current user
            train_loss += np.sqrt(loss.data * mean_corrector)
            s += 1.
            #apply optimizer to update weights, decides the amount of weight udpates
            optimizer.step()
    if epoch % 10 == 0:
        print('epoch: '+str(epoch) + ' loss: ' + str(train_loss/s))

""" SAE testing"""

#evaluation
#loop through each users
test_loss = 0
s = 0.
for id_user in range(nb_users):
    #keep using training set
    input = Variable(training_set[id_user]).unsqueeze(0)
    #create target by copying input
    target = Variable(test_set[id_user]).unsqueeze(0)
    #only look at users who rated at least 1 movie
    if torch.sum(target.data > 0) > 0:
        #get output from the network, a vector of predicted value
        output = sae(input)
        #do not compute gradient with respect to target
        target.require_grad = False
        #don't account the output whose initial input is 0
        output[target == 0] = 0
        loss = criterion(output, target)
        #make demonitor is not zero, to add a small number
        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)
        
        
        #access the data of loss object .data[0]
        #adjust the loss to compute relevant mean for all movies for current user
        test_loss += np.sqrt(loss.data*mean_corrector)
        s += 1.
print('test loss: '+str(test_loss/s))

"""Prediction

comparing the real ratings and the predicted ratings
"""

user_id = 0
movie_title = movies.iloc[:nb_movies, 0:1]
user_rating = training_set.data.numpy()[user_id, :].reshape(-1,1)
user_target = test_set.data.numpy()[user_id, :].reshape(-1,1)

#to be predicted, target
user_target[user_target>0]

user_input = Variable(training_set[user_id]).unsqueeze(0)
# print('training input: ', len(training_set[user_id]), training_set[user_id])
predicted = sae(user_input)
predicted = np.round(predicted.data.numpy().reshape(-1,1), 2)
# print('predicted: \n', len(predicted), predicted)

user_input = user_input.data.numpy().reshape(-1,1)
result_array = np.hstack([movie_title, user_input, user_target, predicted])
result_df = pd.DataFrame(data=result_array, columns=['Movie', 'User input', 'Target Rating', 'Predicted'])

prediction = sae(test_set)
prediction

"""**The task is to predict if a user likes a movie as 1 or dislike as 0.**

Binary data conversion
convert rating 1-5 to binary 1 or 0, as the target is to predict like or not
if not rated, set to 0
if rate is 1 or 2, set as 0 dislike
if rate is >= 3, set as 1 like
"""

##so this is to make input and output data consistent
training_set[training_set == 0] = -1
training_set[training_set == 1] = 0
training_set[training_set == 2] = 0
training_set[training_set >= 3] = 1

test_set[test_set == 0] = -1
test_set[test_set == 1] = 0
test_set[test_set == 2] = 0
test_set[test_set >= 3] = 1

training_set

test_set

""" RBM architecture creation"""

class RBM():
    def __init__(self, nv, nh):
        ##initialize all weights 
        ##a tensor with size of nh, nv in normal dis mean 0 var 1
        self.W = torch.randn(nh, nv)
        #bias for hidden nodes
        #1st dimension is batch, 2nd is num of hidden nodes
        self.a = torch.randn(1, nh)
        #bias for visible nodes
        self.b = torch.randn(1, nv)
    #activate the hidden nodes by sampling all hiddens node, given values of visible nodes 
    def sample_h(self, x):
        #x is values of visible nodes
        #probablity of hiddens h to be activated, given values of visible  nodes v
        wx = torch.mm(x, self.W.t())
        #use sigmoid fuc to activate visible node
        ## a is bias for hidden nodes
        activation = wx + self.a.expand_as(wx)
        ##ith of the vector is the probability of ith hidden nodes to be activated, 
        ##given visible values
        p_h_given_v =torch.sigmoid(activation)
        #samples of all hiddens nodes
        return p_h_given_v, torch.bernoulli(p_h_given_v)
    def sample_v(self, y):
        #y is hidden nodes
        #probablity of visible h to be activated, given hidden  nodes v
        wy = torch.mm(y, self.W)
        #use sigmoid fuc to activate hiddens nodes
        activation = wy + self.b.expand_as(wy)
        ##ith of the vector is the probability of ith visible nodes to be activated, 
        ##given hidden values
        p_v_given_h =torch.sigmoid(activation)
        #samples of all hiddens nodes
        return p_v_given_h, torch.bernoulli(p_v_given_h)
        
    #visible nodes after kth interation
    #probablity of hidden nodes after kth iteration
    def train(self, v0, vk, ph0, phk):
#         self.W += torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)
        self.W += (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()
#         self.W += torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)
        #add zero to keep b as a tensor of 2 dimension
        self.b += torch.sum((v0 - vk), 0)
        self.a += torch.sum((ph0 - phk), 0)

"""Initialize RBM object"""

#number of movies
nv = len(training_set[0])
#number of hidden nodes or num of features
nh = 100
batch_size = 100
rbm = RBM(nv, nh)

"""Model training"""

nb_epoch = 10
for epoch in range(1, nb_epoch+1):
    ##loss function
    train_loss = 0
    #normalize the loss, define a counter
    s = 0.
    #implement a batch learning, 
    for id_user in range(0, nb_users - batch_size, 100):
        #input batch values
        vk = training_set[id_user: id_user+batch_size]
        #target used for loss mesarue: rating 
        v0 = training_set[id_user: id_user+batch_size]
        ##initilize probablity
        #pho: given real rating at begining, probablity of hidden nodes
        ph0, _ = rbm.sample_h(v0)
        #k step of constrative divergence
        for k in range(10):
            _, hk = rbm.sample_h(vk)
            _, vk = rbm.sample_v(hk)
            #training on rating that do exist, rating as -1
            vk[v0<0] = v0[v0<0]
        phk, _ = rbm.sample_h(vk)
        #update weights and bias
        rbm.train(v0, vk, ph0, phk)
        #update train loss
        train_loss += torch.mean(torch.abs(v0[v0>0]-vk[v0>0]))
        s += 1
    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))

"""Test RBM"""

##loss function
test_loss = 0
#normalize the loss, define a counter
s = 0.
#implement a batch learning, 
for id_user in range(0, nb_users):
    #use input of train set to activate RBM
    v_input = training_set[id_user: id_user+1]
    #target used for loss mesarue: rating 
    v_target = test_set[id_user: id_user+1]
    #use only 1 step to make better prediction, though used 10 steps to train
    if len(v_target[v_target>=0]):
        _, h = rbm.sample_h(v_input)
        _, v_input = rbm.sample_v(h)
        #update test loss
        test_loss += torch.mean(torch.abs(v_target[v_target>0]-v_input[v_target>0]))
        s += 1

print('test loss: ' +str(test_loss/s))

"""Prediction"""

prediction = sae(test_set)
prediction



t=prediction.detach().numpy()
t

df3=pd.DataFrame(t)
df3

"""#Pearson correlation coefficient"""

data_table = pd.pivot_table(data,values='rating',columns='item_name',index='user_id')
data_table

"""using the pearson’s correlation coefficient to recommend movies to users based on the movies they liked"""

print("here are a list of 20 movies to recommend to a user who has liked 'Til There Was You (1997)'")
print(df3.corr()[0].sort_values(ascending=False).iloc[:20])

print("here are a list of 10 movies to recommend to a user who has liked '1-900 (1994)'")
print(df3.corr()[1].sort_values(ascending=False).iloc[:10])